#+TITLE: Chapter 5: Dataflow and Reactive Programming: Rediscovering the Wheel
#+AUTHOR: Marcus "Spark" Wellington
#+OPTIONS: toc:nil num:t ^:nil

* Chapter 5: Dataflow and Reactive Programming: Rediscovering the Wheel

#+BEGIN_QUOTE
"Those who do not remember PARC are condemned to reinvent it. Badly."
-- David Thornley, paraphrasing Santayana
#+END_QUOTE

The history of programming paradigms is not always a linear progression of new ideas. Sometimes, it resembles a cycle of forgetting and rediscovery, where fundamental insights emerge, fade from mainstream attention, and then resurface years or decades later—often with new terminology and incomplete understanding of their historical context. Perhaps no paradigm better exemplifies this pattern than dataflow programming, whose core concepts have been repeatedly rediscovered and reimplemented across generations of languages and frameworks.

Dataflow programming's central insight is deceptively simple: model computation as a directed graph of data dependencies, where changes propagate automatically through the system. This model stands in contrast to the control flow emphasis of imperative programming, where the sequence of operations is explicitly specified by the programmer. In a dataflow system, the "when" of computation is determined by data availability and dependency relationships, not by the ordering of statements in a program.

This approach has proven particularly suitable for reactive systems that respond to changing inputs, for parallel computation where dependencies constrain execution ordering, and for modeling systems with complex propagating changes. Yet despite its recurring utility, dataflow programming has repeatedly faded from mainstream attention, only to be reinvented—often with limited awareness of its historical roots.

In this chapter, we'll trace the evolution of dataflow programming from its academic origins through spreadsheets, reactive programming frameworks, and modern stream processing systems. Along the way, we'll examine what has been lost in each cycle of reinvention and what might be gained by a more conscious integration of dataflow concepts into mainstream programming practice.

** Lucid and the Origins of Dataflow

The formal origins of dataflow programming can be traced to the 1970s, with languages like Lucid, developed by Edward Ashcroft and William Wadge. Lucid represented a radical departure from the prevailing imperative paradigm, defining programs not as sequences of state changes but as networks of data dependencies.

In Lucid, variables represent streams of values that evolve over time, and operators define relationships between these streams. This approach, while initially challenging for programmers accustomed to imperative thinking, offered a natural model for certain classes of problems, particularly those involving continuously changing values and complex dependencies.

Consider this simple Lucid program for computing the Fibonacci sequence:

#+BEGIN_SRC
fib = 1 fby (1 fby (fib + next fib));
#+END_SRC

This concise line defines the infinite Fibonacci sequence using two operators: `fby` (followed by) and `next`. The expression can be read as: "The Fibonacci sequence starts with 1, followed by a sequence that starts with 1, followed by the sum of each Fibonacci number and its successor." This declarative definition captures the mathematical essence of the sequence without specifying the step-by-step process for computing it.

Lucid and similar early dataflow languages introduced several key concepts:

1. *Data Dependencies*: Computation is modeled as a graph where nodes represent operations and edges represent data dependencies.

2. *Demand-Driven Evaluation*: Computation proceeds based on what results are needed, not based on a predetermined sequence of operations.

3. *Implicit Parallelism*: Operations without dependencies between them can be executed in parallel without explicit threading code.

4. *Time as a Dimension*: Many dataflow languages incorporated an explicit notion of time or sequencing, where values evolve through a series of states.

The influence of these early dataflow languages extended beyond their direct usage. Dataflow concepts influenced hardware design, leading to experimental dataflow architectures that aimed to exploit the implicit parallelism in dataflow graphs. These architectures, while not commercially successful, advanced our understanding of non-von Neumann computing models and influenced subsequent work in parallel computing.

Despite its elegant approach to certain problems, however, Lucid and other early dataflow languages remained primarily academic. The performance of dataflow implementations on conventional hardware was often disappointing, and the mental model required a significant shift from imperative thinking. The paradigm seemed destined to remain a research curiosity rather than a practical programming model—until it found an unexpected path to mainstream impact.

** Spreadsheets as Successful Dataflow Systems

While academic dataflow languages struggled to gain traction, the dataflow paradigm achieved widespread adoption through a seemingly unrelated development: the electronic spreadsheet. Beginning with VisiCalc in 1979 and evolving through Lotus 1-2-3 to modern spreadsheet applications like Microsoft Excel and Google Sheets, spreadsheets embody the core principles of dataflow programming in a form accessible to millions of non-programmers.

In a spreadsheet, cells can contain values or formulas that reference other cells. When a cell's value changes, all dependent cells are automatically recalculated, with changes propagating through the dependency graph. This model precisely implements the dataflow concept: computation is driven by data dependencies, not by explicitly sequenced operations.

Consider a simple spreadsheet example:

| Cell | Formula        | Current Value |
|------|----------------|---------------|
| A1   | 5              | 5             |
| A2   | 10             | 10            |
| A3   | =A1 + A2       | 15            |
| A4   | =A3 * 2        | 30            |

If we change the value in A1 from 5 to 7, the spreadsheet automatically updates A3 to 17 and A4 to 34. This automatic propagation of changes through the dependency graph is the essence of dataflow programming.

Spreadsheets succeeded where academic dataflow languages struggled for several reasons:

1. *Concrete Visual Model*: Spreadsheets provide a visible grid of cells that makes the dataflow model concrete and manipulable.

2. *Incremental Development*: Users can build spreadsheets cell by cell, seeing immediate results rather than defining complete programs.

3. *Domain Relevance*: The dataflow model naturally suits financial and numerical calculations, which were the primary use cases for early spreadsheets.

4. *Accessibility*: Spreadsheets lowered the barrier to programming, allowing non-programmers to create computational models.

The irony is striking: while computer scientists were developing sophisticated dataflow languages with limited practical impact, the same paradigm was achieving massive adoption through spreadsheets—often without users or even developers recognizing the connection to formal dataflow programming. Spreadsheets became the most successful dataflow programming environment in history, used daily by millions of people who would never identify themselves as programmers.

This success story highlights an important lesson about programming paradigms: their adoption often depends less on theoretical elegance than on accessibility, immediate utility, and alignment with users' mental models. The dataflow concepts that seemed too abstract in languages like Lucid became intuitive when presented in the concrete form of a spreadsheet grid.

** FRP and Modern Reactive Frameworks

While spreadsheets demonstrated the practical value of dataflow concepts for end users, the paradigm remained largely separate from mainstream programming practice. This began to change in the late 1990s with the emergence of Functional Reactive Programming (FRP), initially developed by Conal Elliott and Paul Hudak.

FRP combined functional programming with reactive dataflow concepts, providing a formal model for systems that respond to changing inputs over time. The key insight was representing time-varying values as first-class entities (often called "behaviors" or "signals") that could be composed and transformed using functional operations.

The original FRP work introduced several important concepts:

1. *Continuous Time Model*: Unlike discrete event systems, FRP modeled behaviors as functions over continuous time.

2. *Declarative Composition*: Complex reactive behaviors could be built by composing simpler behaviors using functional operators.

3. *Push-Pull Evaluation*: FRP systems combined push-based notification of changes with pull-based evaluation of dependent values.

4. *Higher-Order Reactivity*: Reactive systems could themselves be reactive, allowing for dynamic creation and composition of reactive behaviors.

These ideas, while powerful, proved challenging to implement efficiently. Early FRP systems suffered from performance issues, particularly around memory usage and update propagation in complex dependency graphs. As a result, FRP remained primarily an academic interest throughout the 2000s, with limited adoption in mainstream programming.

Then, beginning around 2010, a wave of "reactive programming" frameworks emerged—often without explicit acknowledgment of their connection to earlier dataflow and FRP work. Libraries like Rx (Reactive Extensions), React.js, Vue.js, and many others introduced reactive concepts to mainstream programming, typically with simplified models that sacrificed some of FRP's theoretical elegance for practical implementation concerns.

Consider this example in React.js:

#+BEGIN_SRC jsx
function Counter() {
  const [count, setCount] = React.useState(0);
  
  React.useEffect(() => {
    document.title = `You clicked ${count} times`;
  }, [count]);
  
  return (
    <div>
      <p>You clicked {count} times</p>
      <button onClick={() => setCount(count + 1)}>
        Click me
      </button>
    </div>
  );
}
#+END_SRC

This React component defines a user interface that responds to changes in the `count` state variable. When `count` changes (through the `setCount` function), React automatically updates the DOM to reflect the new state. Additionally, the `useEffect` hook specifies that the document title should be updated whenever `count` changes.

This is fundamentally a dataflow system: changes to the `count` variable propagate to both the DOM and the document title based on data dependencies. However, React and similar frameworks typically use discrete event models rather than FRP's continuous time model, and they often implement change propagation through specialized rendering loops rather than general dataflow execution engines.

The reactive programming renaissance has brought dataflow concepts to a wide audience, but often in limited or specialized forms that don't fully capture the generality of the dataflow paradigm. Most reactive frameworks focus primarily on user interface updates or asynchronous event handling, rather than presenting dataflow as a general model for computation.

This specialization has both benefits and costs. On one hand, frameworks like React have made certain dataflow concepts accessible and practically useful for mainstream developers. On the other hand, the connection to the broader dataflow tradition is often obscured, preventing developers from applying these concepts more generally or understanding their full implications.

As with spreadsheets, the most successful applications of dataflow ideas have come not through direct adoption of dataflow languages, but through the incorporation of dataflow concepts into tools and frameworks that address specific practical needs. The loss in this approach is the paradigmatic clarity that might come from a more explicit and general dataflow model.

** The Stream Processing Renaissance

While user interface frameworks were rediscovering reactive programming, another parallel development was bringing dataflow concepts back to mainstream attention: the rise of stream processing systems for handling large-scale data flows, particularly in distributed environments.

Systems like Apache Storm, Spark Streaming, Flink, and Kafka Streams all implement variations on dataflow processing, representing computation as a directed graph of operators that transform, filter, and aggregate streaming data. These systems often use a dataflow execution model where operators are distributed across machines, with data flowing between them according to the dependency graph.

Consider this example in Apache Spark:

#+BEGIN_SRC scala
val lines = spark.readStream.format("kafka").option("subscribe", "topic").load()
val words = lines.as[String].flatMap(_.split(" "))
val wordCounts = words.groupBy("value").count()
val query = wordCounts.writeStream.outputMode("complete").format("console").start()
#+END_SRC

This code defines a streaming computation that reads from a Kafka topic, splits lines into words, counts the occurrences of each word, and outputs the results to the console. The computation is defined as a dataflow graph of transformations, with data flowing from the source through various operators to the output sink.

These stream processing systems share several characteristics with earlier dataflow models:

1. *Graph-Based Computation*: Processing is defined as a directed graph of operators connected by data flows.

2. *Data-Driven Execution*: Computation is triggered by the availability of data, not by explicit control flow.

3. *Declarative Transformations*: Operations are defined in terms of what transformations to apply, not how to execute them.

4. *Automatic Parallelism*: The system automatically parallelizes execution based on the structure of the dataflow graph and available resources.

The stream processing renaissance has brought dataflow concepts to data engineering and analytics, demonstrating the paradigm's value for handling continuous, high-volume data processing. However, as with reactive UI frameworks, these systems often present dataflow as a specialized tool rather than a general programming model.

Moreover, stream processing systems frequently reinvent concepts that were well-established in earlier dataflow work, sometimes with limited awareness of the historical context. Concepts like windowing, event time versus processing time, exactness versus approximation, and handling of late-arriving data were all explored in earlier dataflow research, yet are often presented as novel challenges in stream processing literature.

This pattern of rediscovery without full acknowledgment of historical context represents both a loss and an opportunity. The loss is in potentially repeating mistakes or missing insights from earlier work. The opportunity lies in bringing dataflow concepts to new domains and developers, potentially leading to broader adoption and innovation.

** Time as a First-Class Concept

One of the most profound insights from dataflow programming—and one that is repeatedly rediscovered and then partially forgotten—is the importance of time as a first-class concept in programming systems. Traditional imperative programming treats time implicitly, through the sequencing of operations. Dataflow programming, in contrast, often makes time explicit, modeling how values evolve over time and how changes propagate through a system.

This explicit treatment of time appears in various forms across the dataflow tradition:

1. *Lucid's Streams*: In Lucid, variables represent infinite streams of values evolving over time, with operators that manipulate these streams.

2. *FRP's Behaviors*: Functional Reactive Programming models time-varying values as functions from time to values, allowing for composition and transformation of these time-indexed functions.

3. *Spreadsheet Recalculation*: When a cell changes in a spreadsheet, the system determines which other cells need to be updated, effectively managing the propagation of changes over time.

4. *Event Time in Stream Processing*: Modern stream processing systems distinguish between event time (when an event occurred) and processing time (when the system processes it), allowing for correct handling of out-of-order events.

This focus on time addresses a fundamental challenge in programming: how to reason about systems that evolve and respond to changes over time. Imperative programming handles this through mutable state and carefully sequenced operations—an approach that becomes increasingly complex as systems grow and especially as they become distributed across multiple machines or processes.

Dataflow programming offers an alternative model, where time is not an implicit side effect of operation sequencing but an explicit dimension of the programming model. This explicit treatment of time can lead to more robust handling of concurrent and distributed systems, where the global sequence of operations is not fully under the programmer's control.

Consider how React handles time in UI updates:

#+BEGIN_SRC jsx
function Clock() {
  const [time, setTime] = useState(new Date());
  
  useEffect(() => {
    const timer = setInterval(() => {
      setTime(new Date());
    }, 1000);
    return () => clearInterval(timer);
  }, []);
  
  return <div>Current time: {time.toLocaleTimeString()}</div>;
}
#+END_SRC

In this component, the `time` state is a discrete approximation of a continuously changing value. The React framework handles the propagation of updates from the changing `time` state to the DOM, effectively managing the temporal aspect of the UI's behavior. However, this treatment of time is specialized to UI updates, not a general model for time-varying computation.

A more general and explicit treatment of time would allow programmers to define and compose time-varying values more directly, as in this hypothetical FRP-style code:

#+BEGIN_SRC
val clock = Signal.periodic(1.second).map(_ => new Date())
val displayTime = clock.map(time => time.toLocaleTimeString())
val view = displayTime.map(timeStr => div("Current time: " + timeStr))
#+END_SRC

This approach makes the temporal nature of the computation explicit, modeling the clock as a time-varying signal that can be transformed and combined with other signals. The resulting system is more declarative and potentially more robust to timing variations, as the relationships between time-varying values are defined explicitly rather than emerging implicitly from imperative update logic.

The full implications of making time a first-class concept in programming have yet to be realized in mainstream practice. Each wave of dataflow-inspired systems has captured some aspects of this approach while leaving others unexplored. A more complete integration of explicit temporal semantics into programming languages might address many of the challenges that arise in concurrent, distributed, and reactive systems.

** Conclusion

The history of dataflow programming illustrates a recurring pattern in programming language evolution: fundamental insights emerge, fade from mainstream attention, and then resurface in new forms, often without full awareness of their historical context. This cycle of forgetting and rediscovery represents both a failure of our field's collective memory and a testament to the enduring value of certain programming concepts.

Dataflow programming's core insight—modeling computation as a graph of data dependencies with automatic propagation of changes—has proven remarkably versatile and valuable across domains. From academic languages like Lucid to everyday tools like spreadsheets, from user interface frameworks to distributed stream processing systems, the dataflow model continues to offer an elegant solution to the challenges of managing complex, evolving systems.

Yet this insight has rarely been embraced as a general programming paradigm. Instead, dataflow concepts have been repeatedly specialized for particular domains: financial calculations in spreadsheets, user interface updates in reactive frameworks, large-scale data processing in streaming systems. Each specialization captures some aspects of the dataflow model while omitting others, leading to a fragmented understanding of the paradigm's full potential.

This fragmentation has consequences. Systems that could benefit from a more general dataflow model often reinvent partial solutions, missing opportunities for deeper integration and more elegant designs. Developers familiar with one specialized form of dataflow programming may fail to recognize the same principles in other contexts, limiting their ability to transfer insights across domains.

The treatment of time illustrates this fragmentation clearly. Each dataflow-inspired system develops its own approach to handling temporal aspects of computation, from spreadsheets' immediate recalculation to FRP's continuous-time model to stream processing's event-time semantics. A more unified understanding of time as a dimension in programming might lead to more robust and composable systems across all these domains.

As we continue to build increasingly complex, distributed, and reactive systems, the dataflow paradigm offers valuable guidance. By making data dependencies explicit, by separating the "what" of computation from the "when," and by treating time as a first-class concept, dataflow programming addresses many of the challenges that plague modern software development.

The recurring rediscovery of dataflow concepts suggests that these ideas represent not a historical curiosity but a fundamental insight about computation—one that repeatedly proves its value despite our field's tendency to forget and reinvent. By recognizing this pattern, we can move beyond continual rediscovery toward a more conscious integration of dataflow concepts into mainstream programming practice.

As we transition from examining individual paradigms to exploring what has been lost across programming language evolution, the story of dataflow programming reminds us to look not just forward but also backward—to recognize that the solutions to tomorrow's programming challenges may be found not only in the latest frameworks and languages but also in the forgotten insights of earlier paradigms.

#+BEGIN_QUOTE
"The future is already here—it's just not evenly distributed."
— William Gibson
#+END_QUOTE